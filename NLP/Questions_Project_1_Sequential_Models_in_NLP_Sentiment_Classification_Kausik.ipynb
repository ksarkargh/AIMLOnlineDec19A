{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Questions - Project 1 - Sequential Models in NLP - Sentiment Classification-Kausik.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXaFSkUu0fzm",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1UXScsVx_Wni_JuDdB8LeTnM6jsPfIwkW)\n",
        "\n",
        "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OudB5by50jlI",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "### Dataset\n",
        "- Dataset of 50,000 movie reviews from IMDB, labeled by sentiment positive (1) or negative (0)\n",
        "- Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers).\n",
        "- For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
        "- As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
        "\n",
        "Command to import data\n",
        "- `from tensorflow.keras.datasets import imdb`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxfwbrbuKbk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q34-Y3nRKXdO",
        "colab_type": "text"
      },
      "source": [
        "### Import the data (2 Marks)\n",
        "- Use `imdb.load_data()` method\n",
        "- Get train and test set\n",
        "- Take 10000 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfjMMz-beQVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Take 10000 most frequent words\n",
        "vocab_size= 10000 \n",
        "\n",
        "#load dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ThhD1CCXDkU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "57c175c5-ded4-48a1-ec22-32395cf25373"
      },
      "source": [
        "#After the loading of the data set, lets review it\n",
        "print(\"No of review in the dataset :\",x_train.shape)\n",
        "print(\"word index in the first review item :\",x_train[0])\n",
        "print(\"No of words in the fist review item :\",len(x_train[0]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of review in the dataset : (25000,)\n",
            "word index in the first review item : [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "No of words in the fist review item : 218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DldivBO4LTbP",
        "colab_type": "text"
      },
      "source": [
        "### Pad each sentence to be of same length (2 Marks)\n",
        "- Take maximum sequence length as 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E808XB4tLtic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "#make all sequences of the same length using keras pad_sequences\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#The Maximum Number of word length for each review\n",
        "wordlen = 300  \n",
        "\n",
        "#padding used post to keep dummy word added at the end of the review\n",
        "x_train = pad_sequences(x_train, maxlen=wordlen, padding='post') \n",
        "x_test =  pad_sequences(x_test, maxlen=wordlen, padding='post')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyEeoGUTYYv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "2c7d9950-ae42-47ca-e704-42e75f1b914f"
      },
      "source": [
        "#After the Padding of the data set, lets review it\n",
        "print(\"\\nNo of review in the dataset :\",x_train.shape)\n",
        "print(\"\\nNo of words in the fist review item :\",len(x_train[0]))\n",
        "print(\"\\nwords index in the fist review item : :\",x_train[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "No of review in the dataset : (25000, 300)\n",
            "\n",
            "No of words in the fist review item : 300\n",
            "\n",
            "words index in the fist review item : : [   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
            "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
            "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
            "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
            " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
            "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
            "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
            " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
            "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
            "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
            "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
            "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
            " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
            "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
            "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
            "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfu7PJLAYsF7",
        "colab_type": "text"
      },
      "source": [
        "**After the Padding the Word List vect has padded with 0, and Total words length in a Review is 300. Also, I used padding=\"post\", so that the extra words appears after the original review**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBFFCrybMSXz",
        "colab_type": "text"
      },
      "source": [
        "### Print shape of features & labels (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hdMCUPr7RaCm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c7652092-3f3b-4663-9b92-a771ba622988"
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "print (\"\\nShape of Features(Review) in training set: \", x_train.shape)\n",
        "print (\"\\nShape of Labels in training set: \", y_train.shape)\n",
        "\n",
        "print (\"\\nShape of Features(Review) in test set: \", x_test.shape)\n",
        "print (\"\\nShape of Labels in test set: \", y_test.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shape of Features(Review) in training set:  (25000, 300)\n",
            "\n",
            "Shape of Labels in training set:  (25000,)\n",
            "\n",
            "Shape of Features(Review) in test set:  (25000, 300)\n",
            "\n",
            "Shape of Labels in test set:  (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SliIKD1NZSwv",
        "colab_type": "text"
      },
      "source": [
        "**Out of Total 50000 review, 25000 is used in Training and rest 25000 used for Testing purposes. After padding, Maxmum length of a review items considered to be 300.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOcyRtZfMYZd",
        "colab_type": "text"
      },
      "source": [
        "**Number of review, number of words in each review**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGy6t5GjrtN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7be09dbe-c7b2-4844-bdb7-9e325880e63f"
      },
      "source": [
        "print (\"Number of review - in Training Set: \", (x_train.shape)[0])\n",
        "print (\"Number of review - in Test Set    : \", (x_test.shape)[0])\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of review - in Training Set:  25000\n",
            "Number of review - in Test Set    :  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQX4b08PHISp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "e608a976-cd15-4b56-853a-40a037672238"
      },
      "source": [
        "#Number of words in each review (Considered here the Training Set Only)\n",
        "\n",
        "#Excluding 0 during count as we have already done padding with 0 index\n",
        "list_review_wordscount=([sequence[sequence!=0].size for sequence in x_train])\n",
        "\n",
        "#no of review\n",
        "print(\"Total No of Review item in the List: \",len(list_review_wordscount),'\\n')\n",
        "\n",
        "print(\"Printing the first 10 review word count from the entire Training Set of 25000\\n\")\n",
        "for i in range(0,10): \n",
        "    print(i, list_review_wordscount[i])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total No of Review item in the List:  25000 \n",
            "\n",
            "Printing the first 10 review word count from the entire Training Set of 25000\n",
            "\n",
            "0 218\n",
            "1 189\n",
            "2 141\n",
            "3 300\n",
            "4 147\n",
            "5 43\n",
            "6 123\n",
            "7 300\n",
            "8 233\n",
            "9 130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdkFCZBXmEy_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "26e7f504-8981-42cb-85ab-e7f3cfbcadbf"
      },
      "source": [
        "#Number of words in each review (Considered here the Test Set Only)\n",
        "\n",
        "#Excluding 0 during count as we have already done padding with 0 index\n",
        "list_review_wordscount=([sequence[sequence!=0].size for sequence in x_test])\n",
        "\n",
        "#no of review\n",
        "print(\"Total No of Review item in the List: \",len(list_review_wordscount),'\\n')\n",
        "\n",
        "print(\"Printing the first 10 review word count from the entire Training Set of 25000\\n\")\n",
        "for i in range(0,10): \n",
        "    print(i, list_review_wordscount[i])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total No of Review item in the List:  25000 \n",
            "\n",
            "Printing the first 10 review word count from the entire Training Set of 25000\n",
            "\n",
            "0 68\n",
            "1 260\n",
            "2 300\n",
            "3 181\n",
            "4 108\n",
            "5 132\n",
            "6 300\n",
            "7 180\n",
            "8 134\n",
            "9 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrIPRjwPZ-XS",
        "colab_type": "text"
      },
      "source": [
        "I have **printed the first 10 review word count from both Training and Test set**, just to discard long list of review items, we can print the entire list in a by selecting the range as the entire x_train list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cNk5sDvMr3j",
        "colab_type": "text"
      },
      "source": [
        "**Number of labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z00-mYgMoKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1ac2f500-94d1-4cae-b98f-bd97e3208632"
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(\"No of Label in Training Set \", np.unique(y_train))\n",
        "print(\"No of Label in Test Set     \", np.unique(y_test))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of Label in Training Set  [0 1]\n",
            "No of Label in Test Set      [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB1lKS7nNKrg",
        "colab_type": "text"
      },
      "source": [
        "Label **0 - Negetive** And Label **1 Positive** Two different Label exists in the Dataset for Both Training and Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdXPWuOmNEbh",
        "colab_type": "text"
      },
      "source": [
        "### Print value of any one feature and it's label (2 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGLEdeFmNZfR",
        "colab_type": "text"
      },
      "source": [
        "**Feature value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKFyMa28zztL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "16cbb34a-bf63-4be1-9f61-eae275226b0b"
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "rev_no=11 #Lets a random review say 11\n",
        "print(\"Feature:\", x_test[rev_no])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature: [   1   54   13   86  219   14   20   11    4  750   13   16   38 1612\n",
            "   12  340 4280   11   61  652   13  161   67   12   18    6 2068   95\n",
            "  872   51    4  609  903   67  146  149   32    2  102  150    8   67\n",
            "  121   12  435  355   61  482    9   12   16   19  755  457   15   16\n",
            "    4   86    8    2    4  226   13  244   11    6  925    2   13   67\n",
            "  916  538  449    2   51    9 1448  449   94    6  925  449   94   24\n",
            "    6  925  449  858   13   67  142 3642  449  115  330    2  769  148\n",
            " 2289   92   60 5276    4  953    8   30 3057   42 1231    8    4 2269\n",
            "   39    4   86  470  102   15   48   25  219    2   25   26  184   76\n",
            " 5822    5  351    4   86  342    2    8   14  769   63   93   12   38\n",
            "  629   11    4   86  273  164  939  164  916    4  953  188 3057 4572\n",
            "   36  385   16    4   64   31   15  100 4866   41   96   46    7   12\n",
            "   86   88    7 1697 1265   95   88   59   69 1618   44    4   14   20\n",
            "   33  222 1027    8 1231    8   32   15   60  151   12   16    6   78\n",
            "   65   12   16  131  643    2   57  146  184  252  173  457   16    4\n",
            "   86  173    8  340    5 1254 2330    4    2  201  150   36   26    2\n",
            "  300    5   13   92   60  104   13   80  106   12  449   37  244   13\n",
            " 3988   13  242   80    5  242   80   30  685  174    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_85Hqm0Nb1I",
        "colab_type": "text"
      },
      "source": [
        "**Label value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FoehB5jNd1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29e83cc5-1deb-4d9e-8677-61751bf480db"
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "print(\"Label:\", y_test[[rev_no]])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1lA18V9sgIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce20294a-4c6a-447f-8b5c-f7c06733c3b0"
      },
      "source": [
        "#Lets review with the provided data\n",
        "y_test[0:15]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cof4LSxNxuv",
        "colab_type": "text"
      },
      "source": [
        "### Decode the feature value to get original sentence (2 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_oiAyPZOkJD",
        "colab_type": "text"
      },
      "source": [
        "***First, retrieve a dictionary that contains mapping of words to their index in the IMDB dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clsk-yK8OtzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "# load the dictionary mappings from word to integer index\n",
        "word_index_dict = imdb.get_word_index()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRgOD5S2Uuvd",
        "colab_type": "text"
      },
      "source": [
        "***Now use the dictionary to get the original words from the encodings, for a particular sentence***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ504QDORwxj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "284cdb74-1fa6-42ed-b0d4-42b797facc6e"
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "# reverse word index to map integer indexes to their respective words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index_dict.items()])\n",
        "\n",
        "#decode the review, mapping integer indices to words\n",
        "# indices are off by 3 because 0, 1, and 2 are reserverd indices for \"padding\", \"Start of sequence\" and \"unknown\"\n",
        "\n",
        "orig_review = ' '.join([reverse_word_index.get(i-3, '?') for i in x_test[rev_no]])\n",
        "print(\"Original Review Content:\\n\")\n",
        "\n",
        "orig_review"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Review Content:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"? when i first saw this movie in the theater i was so angry it completely blew in my opinion i didn't see it for a decade then decided what the hell let's see i'm watching all ? movies now to see where it went wrong my guess is it was with sequel 5 that was the first to ? the whole i am in a dream ? i see weird stuff oh ? what is happening oh its a dream oh its not a dream oh wait i see something spooky oh never mind ? storyline those sequels don't even require the box to be opened or stick to the rules from the first 4 movies that if you saw ? you are pretty much screwed and dead the first 3 ? to this storyline which made it so scary in the first place nothing fantasy nothing weird the box got opened boom they came was the only one that could bargain her way out of it first because of uncle frank then because she had information about the this movie at least attempts to stick to all that even though it was a bad story it was still somewhat ? no i'm pretty sure part 5 was the first part to completely and utterly destroy the ? series now they are ? 1 and i don't even think i will watch it oh who am i kidding i probably will and probably will be disappointed again ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLGABrJoVZe6",
        "colab_type": "text"
      },
      "source": [
        "***Get the sentiment for the above sentence***\n",
        "- positive (1)\n",
        "- negative (0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBtGE1mfuB7B",
        "colab_type": "text"
      },
      "source": [
        "## Defining a Baseline Model:\n",
        "When we work with machine learning, one important step is to define a baseline model. This usually involves **a simple model**, ***which is then used as a comparison with the more advanced models that you want to test.*** In this case, I have used **the baseline model as LogisticRegression model** to compare it to the more advanced methods involving (deep) neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDyQGJT0Ve-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8f6e72cf-b9f2-4c6c-e9e1-ec1e3a39171b"
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "# The classification model we are going to use is the logistic regression which is a simple yet powerful linear model \n",
        "# that is mathematically speaking in fact a form of regression between 0 and 1 based on the input feature vector.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(x_train, y_train)\n",
        "score = classifier.score(x_test, y_test)\n",
        "print(score)\n",
        "y_pred = classifier.predict(x_test[[rev_no]])\n",
        "print(y_pred)\n",
        "if(y_pred[0] == 1):\n",
        "  print(\"Positive: \", y_pred[0]) \n",
        "else:\n",
        "  print(\"Negative: \", y_pred[0]) "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.50376\n",
            "[0]\n",
            "Negative:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUgjCVe0vG6O",
        "colab_type": "text"
      },
      "source": [
        "**In the above review(Rev No 11)** its a mix review with more negative senses, \n",
        "\n",
        "*   though it was a bad story\n",
        "*   nothing fantasy nothing weird\n",
        "\n",
        "posibly got a Negative review.\n",
        "\n",
        "With the **Baseline Model of LogisticRegression** the **prediction is correct** but **accuracy score is poor with 50.38%**, With this accuracy most of the other reviews are not getting predicted correctly and is not matching.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmCjr8miXIWB",
        "colab_type": "text"
      },
      "source": [
        "## Define model (10 Marks)\n",
        "- Define a Sequential Model\n",
        "- Add Embedding layer\n",
        "  - Embedding layer turns positive integers into dense vectors of fixed size\n",
        "  - `tensorflow.keras` embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unique integer number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn LabelEncoder.\n",
        "  - Size of the vocabulary will be 10000\n",
        "  - Give dimension of the dense embedding as 100\n",
        "  - Length of input sequences should be 300\n",
        "- Add LSTM layer\n",
        "  - Pass value in `return_sequences` as True\n",
        "- Add a `TimeDistributed` layer with 100 Dense neurons\n",
        "- Add Flatten layer\n",
        "- Add Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np5GxT1caFEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Dropout, Embedding, TimeDistributed, Bidirectional\n",
        "\n",
        "#We already Have the information Given as below\n",
        "\n",
        "#Size of the vocabulary will be 10000\n",
        "vocab_size= 10000\n",
        "\n",
        "#Give dimension of the dense embedding as 100\n",
        "embedDim=100\n",
        "\n",
        "#Length of input sequences should be 300\n",
        "max_review_len = 300\n",
        "\n",
        "# Define a Sequential Model\n",
        "model = Sequential()\n",
        "\n",
        "#Add Embedding layer\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedDim, input_length=max_review_len))\n",
        "\n",
        "#Add a Bidrectional LSTM Layer with Dropout=0.2\n",
        "model.add(Bidirectional(LSTM(units=100, return_sequences=True, dropout=0.2), merge_mode=\"concat\")) # Default Activation Function is \"tanh\" is used here\n",
        "\n",
        "#Add a TimeDistributed layer with 100 Dense neurons(Already mentioned as required)\n",
        "model.add(TimeDistributed(Dense(100, activation=\"relu\")))\n",
        "\n",
        "#Add Flatten layer\n",
        "model.add(Flatten())\n",
        "\n",
        "#Add Dense layer\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#Output Layer\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc4bknOobDby",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model (2 Marks)\n",
        "- Use Optimizer as Adam\n",
        "- Use Binary Crossentropy as loss\n",
        "- Use Accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw4RJ0CQbwFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sEzwazqbz3T",
        "colab_type": "text"
      },
      "source": [
        "### Print model summary (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx1yxwlb2Ue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "4dc6a57b-0277-4ea1-99ef-9ae58093a711"
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 300, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 300, 200)          160800    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 300, 100)          20100     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 30000)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 50)                1500050   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 2,681,001\n",
            "Trainable params: 2,681,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmkolKP4b-U6",
        "colab_type": "text"
      },
      "source": [
        "### Fit the model (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRg3KFXLcAkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "62957ca7-40d5-4da4-cc09-0f85ed66f275"
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.001, verbose=1, mode='min')\n",
        "callbacks_list = [earlystop]\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.3, callbacks=callbacks_list) #Lets Use 30% of Training Data as Validation Data"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "547/547 [==============================] - 23s 43ms/step - loss: 0.3826 - accuracy: 0.8294 - val_loss: 0.2622 - val_accuracy: 0.8932\n",
            "Epoch 2/5\n",
            "547/547 [==============================] - 23s 41ms/step - loss: 0.1993 - accuracy: 0.9252 - val_loss: 0.2715 - val_accuracy: 0.8932\n",
            "Epoch 00002: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb19b69c6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwLl54MXnkEA",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate model (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EUqY-bD8RaDR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2647cfea-00c9-4931-a23f-44bc0cf358a4"
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "scores = model.evaluate(x_test, y_test, batch_size = 32, verbose= 1)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 9s 12ms/step - loss: 0.2904 - accuracy: 0.8805\n",
            "Accuracy: 88.05%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2amr1tJn9Jz",
        "colab_type": "text"
      },
      "source": [
        "### Predict on one sample (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdbXlqq17W6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7d7e05dd-4cd1-40bd-fa69-a7e904656f78"
      },
      "source": [
        "#### Add your code here ####\n",
        "\n",
        "y_pred = model.predict(x_test[[rev_no]]) #Lets predict the Same review we Have used in Baseline Model rev_no=11\n",
        "\n",
        "pred=int(y_pred[0][0])\n",
        "\n",
        "print(\"Predicting for the Movie Review Number: \",rev_no,'\\n')\n",
        "\n",
        "if(pred == 1):\n",
        "  print(\"Movie Sentiment is : Positive (\", pred,')') \n",
        "else:\n",
        "  print(\"Movie Sentiment is : Negative (\", pred,')') "
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting for the Movie Review Number:  11 \n",
            "\n",
            "Movie Sentiment is : Negative ( 0 )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxkaGbUYi9yP",
        "colab_type": "text"
      },
      "source": [
        "**The prediction is Negative** for the Same review(rev_no=11) we evaluated in the BaseModel, **However this time the accuracuy is Higher compared to earlier 50.37% to 88.05%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0Z_WVf0uIxG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}